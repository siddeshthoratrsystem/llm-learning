{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7865\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7865/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt to LLM: \n",
      "    Answer Clearly: \n",
      "\n",
      "    \"hi\"\n",
      "    \n",
      "LLM Response: content='Hello! How can I assist you today?' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 18, 'total_tokens': 27, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_c4585b5b9c', 'id': 'chatcmpl-Cyca2QAt8SY51nai82M9YKE445FIL', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='lc_run--019bc6a1-4843-7662-929f-783819337123-0' tool_calls=[] invalid_tool_calls=[] usage_metadata={'input_tokens': 18, 'output_tokens': 9, 'total_tokens': 27, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "Checking for feedback...\n",
      "Requesting feedback from user.\n",
      "Prompt to LLM: \n",
      "    Answer Clearly: \n",
      "\n",
      "    \"hi\"\n",
      "    \n",
      "LLM Response: content='Hello! How can I assist you today?' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 18, 'total_tokens': 27, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_c4585b5b9c', 'id': 'chatcmpl-CycaJliMK8Y0lfMjvdT19jgDTZPdF', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='lc_run--019bc6a1-8b4f-7e43-a31c-02460b250f20-0' tool_calls=[] invalid_tool_calls=[] usage_metadata={'input_tokens': 18, 'output_tokens': 9, 'total_tokens': 27, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "Checking for feedback...\n",
      "Storing feedback...\n",
      "Current Feedback Store: [{'question': 'hi', 'answer': 'Hello! How can I assist you today?', 'rating': 2, 'comment': 'some comment'}]\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# INSTALL (run once)\n",
    "# ===============================\n",
    "# !pip install -U gradio langgraph langchain langchain-openai\n",
    "\n",
    "import gradio as gr\n",
    "from typing import TypedDict, Optional, List, Dict\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.types import interrupt\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# ===============================\n",
    "# GLOBAL FEEDBACK STORE\n",
    "# ===============================\n",
    "FEEDBACK_STORE: List[Dict] = []\n",
    "\n",
    "# ===============================\n",
    "# STATE\n",
    "# ===============================\n",
    "class QAState(TypedDict):\n",
    "    question: str\n",
    "    answer: Optional[str]\n",
    "    feedbackRating: Optional[int]\n",
    "    feedbackComment: Optional[str]\n",
    "\n",
    "# ===============================\n",
    "# LLM\n",
    "# ===============================\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "# ===============================\n",
    "# LANGGRAPH NODES\n",
    "# ===============================\n",
    "\n",
    "def answer_question(state: QAState) -> QAState:\n",
    "    prompt = f'''\n",
    "    Answer Clearly: \n",
    "\n",
    "    \"{state['question']}\"\n",
    "    '''\n",
    "\n",
    "    print(\"Prompt to LLM:\", prompt)\n",
    "    response = llm.invoke([{\"role\": \"user\", \"content\": prompt}])\n",
    "\n",
    "    print(\"LLM Response:\", response)\n",
    "    state[\"answer\"] = response.content\n",
    "    return state\n",
    "\n",
    "\n",
    "def feedback_interrupt(state: QAState) -> QAState:\n",
    "    print(\"Checking for feedback...\")\n",
    "    if state.get(\"feedbackRating\") is None:\n",
    "        print(\"Requesting feedback from user.\")\n",
    "        interrupt({\n",
    "            \"type\": \"feedback_request\",\n",
    "            \"answer\": state[\"answer\"]\n",
    "        })\n",
    "        print(\"Feedback received:\", state.get(\"feedbackRating\"), state.get(\"feedbackComment\"))\n",
    "    return state\n",
    "\n",
    "\n",
    "def store_feedback(state: QAState) -> QAState:\n",
    "    print(\"Storing feedback...\")\n",
    "    FEEDBACK_STORE.append({\n",
    "        \"question\": state[\"question\"],\n",
    "        \"answer\": state[\"answer\"],\n",
    "        \"rating\": state[\"feedbackRating\"],\n",
    "        \"comment\": state[\"feedbackComment\"]\n",
    "    })\n",
    "    print(\"Current Feedback Store:\", FEEDBACK_STORE)\n",
    "    return state\n",
    "\n",
    "# ===============================\n",
    "# GRAPH\n",
    "# ===============================\n",
    "builder = StateGraph(QAState)\n",
    "\n",
    "builder.add_node(\"answer\", answer_question)\n",
    "builder.add_node(\"feedback_interrupt\", feedback_interrupt)\n",
    "builder.add_node(\"store_feedback\", store_feedback)\n",
    "\n",
    "builder.set_entry_point(\"answer\")\n",
    "builder.add_edge(\"answer\", \"feedback_interrupt\")\n",
    "builder.add_edge(\"feedback_interrupt\", \"store_feedback\")\n",
    "builder.add_edge(\"store_feedback\", END)\n",
    "\n",
    "graph = builder.compile()\n",
    "\n",
    "# ===============================\n",
    "# GRADIO LOGIC\n",
    "# ===============================\n",
    "\n",
    "session_state = {}\n",
    "\n",
    "def chat_fn(user_input, history):\n",
    "    state = {\n",
    "        \"question\": user_input,\n",
    "        \"answer\": None,\n",
    "        \"feedbackRating\": None,\n",
    "        \"feedbackComment\": None\n",
    "    }\n",
    "\n",
    "    result = graph.invoke(state)\n",
    "    session_state[\"state\"] = result\n",
    "\n",
    "    history.append({\"role\": \"user\", \"content\": user_input})\n",
    "    history.append({\"role\": \"assistant\", \"content\": result[\"answer\"]})\n",
    "\n",
    "    return history, gr.update(visible=True)\n",
    "\n",
    "\n",
    "def submit_feedback(rating, comment, history):\n",
    "    state = session_state[\"state\"]\n",
    "    state[\"feedbackRating\"] = rating\n",
    "    state[\"feedbackComment\"] = comment\n",
    "\n",
    "    graph.invoke(state)\n",
    "\n",
    "    history.append({\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": f\"üìù Thanks for the feedback! ‚≠ê {rating}/5\"\n",
    "    })\n",
    "\n",
    "    return history, gr.update(visible=False), None, \"\"\n",
    "\n",
    "# ===============================\n",
    "# GRADIO UI\n",
    "# ===============================\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"## ü§ñ Q&A Chatbot with Feedback Loop\")\n",
    "\n",
    "    # chatbot = gr.Chatbot()\n",
    "    chatbot = gr.Chatbot()\n",
    "    user_input = gr.Textbox(placeholder=\"Ask a question...\")\n",
    "    send_btn = gr.Button(\"Send\")\n",
    "\n",
    "    with gr.Group(visible=False) as feedback_box:\n",
    "        gr.Markdown(\"### ‚≠ê Rate the answer\")\n",
    "        rating = gr.Slider(1, 5, step=1, label=\"Rating\")\n",
    "        comment = gr.Textbox(label=\"Optional comment\")\n",
    "        submit_btn = gr.Button(\"Submit Feedback\")\n",
    "\n",
    "    send_btn.click(\n",
    "        chat_fn,\n",
    "        inputs=[user_input, chatbot],\n",
    "        outputs=[chatbot, feedback_box]\n",
    "    )\n",
    "\n",
    "    submit_btn.click(\n",
    "        submit_feedback,\n",
    "        inputs=[rating, comment, chatbot],\n",
    "        outputs=[chatbot, feedback_box, rating, comment]\n",
    "    )\n",
    "\n",
    "demo.launch()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
